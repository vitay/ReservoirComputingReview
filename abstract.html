<div>The brain is a highly complex, recurrently structured dynamical system. Computational models are essential to understand its dynamic behavior, since mere experimental observation comes with spatiotemporal limitations regarding the brain states that can be observed and cannot deliver mechanistic insights by itself. In recent years, reservoir computing has become a popular modeling framework for the investigation of various brain systems and phenomena. Initially introduced by H. JÃ¤ger and W. Maas, reservoir computing comprises the family of randomly connected recurrent neural networks (RNNs) and has been shown to have universal computation properties. While it has originally been proposed as a machine learning concept, it has proven useful for gaining insights into structural and functional organization principles of the brain. Within this article, we review the literature that employed reservoir computing architectures to study  brain systems. Thereby,  we make a distinction between reservoir computing and deep learning approaches. Our main goal is to identify spatial, temporal and functional scales on which brain systems can be understood as a reservoir, i.e. on which certain measures of brain dynamics follow reservoir computing principles. For those that do so possible to generate clear predictions regarding computational properties, memory capacities and learning performance of these brain systems that could be tested experimentally. We classify the body of literature into articles that study a) basic neural dynamics, b) structural organization principles and c) high-level brain functions via reservoir computing.&nbsp;</div>