@article{Miconi_2017,
  doi = {10.7554/elife.20899},
  url = {https://doi.org/10.7554%2Felife.20899},
  year = {2017},
  month = {feb},
  volume = {6},
  author = {Thomas Miconi},
  title = {{Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks}},
  journal = {{eLife}},
}


@article{Song2017,
  title = {{Reward-based training of recurrent neural networks for cognitive and value-based tasks.}},
  date = {2017 Jan 13},
  source = {Elife},
  authors = {Song, HF and Yang, GR and Wang, XJ},
  author = {Song, HF and Yang, GR and Wang, XJ},
  year = {2017},
  month = {Jan},
  journal = {Elife},
  volume = {6},
  number = {},
  pages = {},
  pubmed_id = {28084991},
}


@article{Jaeger_2007,
  doi = {10.4249/scholarpedia.2330},
  url = {https://doi.org/10.4249%2Fscholarpedia.2330},
  year = {2007},
  publisher = {Scholarpedia},
  volume = {2},
  number = {9},
  pages = {2330},
  author = {Herbert Jaeger},
  title = {{Echo state network}},
  journal = {Scholarpedia},
}


@article{Maass_2002,
  doi = {10.1162/089976602760407955},
  url = {https://doi.org/10.1162%2F089976602760407955},
  year = {2002},
  month = {nov},
  publisher = {{MIT} Press - Journals},
  volume = {14},
  number = {11},
  pages = {2531--2560},
  author = {Wolfgang Maass and Thomas Natschläger and Henry Markram},
  title = {{Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations}},
  journal = {Neural Computation},
}


@inproceedings{Jaeger,
  doi = {10.1109/ijcnn.2005.1556090},
  url = {https://doi.org/10.1109%2Fijcnn.2005.1556090},
  publisher = {{IEEE}},
  author = {H. Jaeger},
  title = {{Reservoir riddles: suggestions for echo state network research (extended abstract)}},
  booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks 2005.},
}


@article{Jaeger_2007a,
  doi = {10.1016/j.neunet.2007.04.001},
  url = {https://doi.org/10.1016%2Fj.neunet.2007.04.001},
  year = {2007},
  month = {apr},
  publisher = {Elsevier {BV}},
  volume = {20},
  number = {3},
  pages = {287--289},
  author = {Herbert Jaeger and Wolfgang Maass and Jose Principe},
  title = {{Special issue on echo state networks and liquid state machines}},
  journal = {Neural Networks},
}


@article{Luko_evi_ius_2009,
  doi = {10.1016/j.cosrev.2009.03.005},
  url = {https://doi.org/10.1016%2Fj.cosrev.2009.03.005},
  year = {2009},
  month = {aug},
  publisher = {Elsevier {BV}},
  volume = {3},
  number = {3},
  pages = {127--149},
  author = {Mantas Luko{\v{s}}evi{\v{c}}ius and Herbert Jaeger},
  title = {{Reservoir computing approaches to recurrent neural network training}},
  journal = {Computer Science Review},
}


@article{Luko_evi_ius_2012,
  doi = {10.1007/s13218-012-0204-5},
  url = {https://doi.org/10.1007%2Fs13218-012-0204-5},
  year = {2012},
  month = {may},
  publisher = {Springer Nature},
  volume = {26},
  number = {4},
  pages = {365--371},
  author = {Mantas Luko{\v{s}}evi{\v{c}}ius and Herbert Jaeger and Benjamin Schrauwen},
  title = {{Reservoir Computing Trends}},
  journal = {{KI} - Künstliche Intelligenz},
}


@inproceedings{Goodman,
  doi = {10.1109/ijcnn.2006.1716628},
  url = {https://doi.org/10.1109%2Fijcnn.2006.1716628},
  publisher = {{IEEE}},
  author = {E. Goodman and D. Ventura},
  title = {{Spatiotemporal Pattern Recognition via Liquid State Machines}},
  booktitle = {The 2006 {IEEE} International Joint Conference on Neural Network Proceedings},
}


@inproceedings{Norton,
  doi = {10.1109/ijcnn.2006.1716685},
  url = {https://doi.org/10.1109%2Fijcnn.2006.1716685},
  publisher = {{IEEE}},
  author = {D. Norton and D. Ventura},
  title = {{Preparing More Effective Liquid State Machines Using Hebbian Learning}},
  booktitle = {The 2006 {IEEE} International Joint Conference on Neural Network Proceedings},
}


@incollection{Maass_2011,
  doi = {10.1142/9781848162778_0008},
  url = {https://doi.org/10.1142%2F9781848162778_0008},
  year = {2011},
  month = {feb},
  publisher = {{IMPERIAL} {COLLEGE} {PRESS}},
  pages = {275--296},
  author = {Wolfgang Maass},
  title = {{Liquid State Machines: Motivation Theory, and Applications}},
  booktitle = {Computability in Context},
}


@incollection{2015,
  doi = {10.1007/978-1-4614-6675-8_100309},
  url = {https://doi.org/10.1007%2F978-1-4614-6675-8_100309},
  year = {2015},
  publisher = {Springer New York},
  pages = {1500--1500},
  title = {{Liquid State Machines}},
  booktitle = {Encyclopedia of Computational Neuroscience},
}


@article{Werbos_1990,
  doi = {10.1109/5.58337},
  url = {https://doi.org/10.1109%2F5.58337},
  year = {1990},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {78},
  number = {10},
  pages = {1550--1560},
  author = {P.J. Werbos},
  title = {{Backpropagation through time: what it does and how to do it}},
  journal = {Proceedings of the {IEEE}},
}


@article{Atiya_2000,
  doi = {10.1109/72.846741},
  url = {https://doi.org/10.1109%2F72.846741},
  year = {2000},
  month = {may},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {11},
  number = {3},
  pages = {697--709},
  author = {A.F. Atiya and A.G. Parlos},
  title = {{New results on recurrent network training: unifying the algorithms and accelerating convergence}},
  journal = {{IEEE} Transactions on Neural Networks},
}


@inproceedings{Steil,
  doi = {10.1109/ijcnn.2004.1380039},
  url = {https://doi.org/10.1109%2Fijcnn.2004.1380039},
  publisher = {{IEEE}},
  author = {J.J. Steil},
  title = {{Backpropagation-decorrelation: online recurrent learning with O(N) complexity}},
  booktitle = {2004 {IEEE} International Joint Conference on Neural Networks ({IEEE} Cat. No.04CH37541)},
}


@article{Steil_2006,
  doi = {10.1016/j.neucom.2005.12.012},
  url = {https://doi.org/10.1016%2Fj.neucom.2005.12.012},
  year = {2006},
  month = {mar},
  publisher = {Elsevier {BV}},
  volume = {69},
  number = {7-9},
  pages = {642--650},
  author = {Jochen J. Steil},
  title = {{Online stability of backpropagation{\textendash}decorrelation recurrent learning}},
  journal = {Neurocomputing},
}


@incollection{Lazar_2011,
  doi = {10.1007/978-3-642-21738-8_17},
  url = {https://doi.org/10.1007%2F978-3-642-21738-8_17},
  year = {2011},
  publisher = {Springer Berlin Heidelberg},
  pages = {127--134},
  author = {Andreea Lazar and Gordon Pipa and Jochen Triesch},
  title = {{Emerging Bayesian Priors in a Self-Organizing Recurrent Network}},
  booktitle = {Lecture Notes in Computer Science},
}


@article{Lazar_2009,
  doi = {10.3389/neuro.10.019.2009},
  url = {https://doi.org/10.3389%2Fneuro.10.019.2009},
  year = {2009},
  publisher = {Frontiers Media {SA}},
  volume = {3},
  author = {Andreea Lazar},
  title = {{{SORN}: a self-organizing recurrent neural network}},
  journal = {Frontiers in Computational Neuroscience},
}


@article{Bourjaily_2011,
  doi = {10.3389/fncom.2011.00037},
  url = {https://doi.org/10.3389%2Ffncom.2011.00037},
  year = {2011},
  publisher = {Frontiers Media {SA}},
  volume = {5},
  author = {Mark A. Bourjaily and Paul Miller},
  title = {{Excitatory Inhibitory, and Structural Plasticity Produce Correlated Connectivity in Random Networks Trained to Solve Paired-Stimulus Tasks}},
  journal = {Frontiers in Computational Neuroscience},
}


@article{Klampfl_2013,
  doi = {10.1523/jneurosci.5044-12.2013},
  url = {https://doi.org/10.1523%2Fjneurosci.5044-12.2013},
  year = {2013},
  month = {jul},
  publisher = {Society for Neuroscience},
  volume = {33},
  number = {28},
  pages = {11515--11529},
  author = {S. Klampfl and W. Maass},
  title = {{Emergence of Dynamic Memory Traces in Cortical Microcircuit Models through {STDP}}},
  journal = {Journal of Neuroscience},
}


@article{Galtier_2014,
  doi = {10.1016/j.neunet.2014.04.002},
  url = {https://doi.org/10.1016%2Fj.neunet.2014.04.002},
  year = {2014},
  month = {aug},
  publisher = {Elsevier {BV}},
  volume = {56},
  pages = {10--21},
  author = {Mathieu N. Galtier and Camille Marini and Gilles Wainrib and Herbert Jaeger},
  title = {{Relative entropy minimizing noisy non-linear neural network to approximate stochastic processes}},
  journal = {Neural Networks},
}


@article{Kappel_2015,
  doi = {10.1371/journal.pcbi.1004485},
  url = {https://doi.org/10.1371%2Fjournal.pcbi.1004485},
  year = {2015},
  month = {nov},
  publisher = {Public Library of Science ({PLoS})},
  volume = {11},
  number = {11},
  pages = {e1004485},
  author = {David Kappel and Stefan Habenschuss and Robert Legenstein and Wolfgang Maass},
  editor = {Jeff Beck},
  title = {{Network Plasticity as Bayesian Inference}},
  journal = {{PLOS} Computational Biology},
}


@article{Kappel_2018,
  doi = {10.1523/eneuro.0301-17.2018},
  url = {https://doi.org/10.1523%2Feneuro.0301-17.2018},
  year = {2018},
  publisher = {Society for Neuroscience},
  volume = {5},
  number = {2},
  pages = {ENEURO.0301--17.2018},
  author = {David Kappel and Robert Legenstein and Stefan Habenschuss and Michael Hsieh and Wolfgang Maass},
  title = {{A Dynamic Connectome Supports the Emergence of Stable Computational Function of Neural Circuits through Reward-Based Learning}},
  journal = {eneuro},
}


@article{Aswolinskiy_2015,
  doi = {10.3389/fncom.2015.00036},
  url = {https://doi.org/10.3389%2Ffncom.2015.00036},
  year = {2015},
  month = {mar},
  publisher = {Frontiers Media {SA}},
  volume = {9},
  author = {Witali Aswolinskiy and Gordon Pipa},
  title = {{{RM}-{SORN}: a reward-modulated self-organizing recurrent neural network}},
  journal = {Frontiers in Computational Neuroscience},
}


@article{Berger_2017,
  doi = {10.1038/s41598-017-11424-5},
  url = {https://doi.org/10.1038%2Fs41598-017-11424-5},
  year = {2017},
  month = {sep},
  publisher = {Springer Nature},
  volume = {7},
  number = {1},
  author = {Damian L. Berger and Lucilla de Arcangelis and Hans J. Herrmann},
  title = {{Spatial features of synaptic adaptation affecting learning performance}},
  journal = {Scientific Reports},
}


@article{Hoerzer_2012,
  doi = {10.1093/cercor/bhs348},
  url = {https://doi.org/10.1093%2Fcercor%2Fbhs348},
  year = {2012},
  month = {nov},
  publisher = {Oxford University Press ({OUP})},
  volume = {24},
  number = {3},
  pages = {677--690},
  author = {Gregor M. Hoerzer and Robert Legenstein and Wolfgang Maass},
  title = {{Emergence of Complex Computational Structures From Chaotic Neural Networks Through Reward-Modulated Hebbian Learning}},
  journal = {Cerebral Cortex},
}


@article{Jonke_2017,
  doi = {10.1523/jneurosci.2078-16.2017},
  url = {https://doi.org/10.1523%2Fjneurosci.2078-16.2017},
  year = {2017},
  month = {jul},
  publisher = {Society for Neuroscience},
  volume = {37},
  number = {35},
  pages = {8511--8523},
  author = {Zeno Jonke and Robert Legenstein and Stefan Habenschuss and Wolfgang Maass},
  title = {{Feedback Inhibition Shapes Emergent Computational Properties of Cortical Microcircuit Motifs}},
  journal = {The Journal of Neuroscience},
}


@article{Schmidhuber_2007,
  doi = {10.1162/neco.2007.19.3.757},
  url = {https://doi.org/10.1162%2Fneco.2007.19.3.757},
  year = {2007},
  month = {mar},
  publisher = {{MIT} Press - Journals},
  volume = {19},
  number = {3},
  pages = {757--779},
  author = {Jürgen Schmidhuber and Daan Wierstra and Matteo Gagliolo and Faustino Gomez},
  title = {{Training Recurrent Networks by Evolino}},
  journal = {Neural Computation},
}


@incollection{Jiang_2008,
  doi = {10.1007/978-3-540-87700-4_22},
  url = {https://doi.org/10.1007%2F978-3-540-87700-4_22},
  year = {2008},
  publisher = {Springer Berlin Heidelberg},
  pages = {215--224},
  author = {Fei Jiang and Hugues Berry and Marc Schoenauer},
  title = {{Supervised and Evolutionary Learning of Echo State Networks}},
  booktitle = {Parallel Problem Solving from Nature {\textendash} {PPSN} X},
}


@article{Zheng_2014,
  doi = {10.3389/fncom.2014.00066},
  url = {https://doi.org/10.3389%2Ffncom.2014.00066},
  year = {2014},
  month = {jun},
  publisher = {Frontiers Media {SA}},
  volume = {8},
  author = {Pengsheng Zheng and Jochen Triesch},
  title = {{Robust development of synfire chains from multiple plasticity mechanisms}},
  journal = {Frontiers in Computational Neuroscience},
}


@article{Kim_2018,
  doi = {10.7554/elife.37124},
  url = {https://doi.org/10.7554%2Felife.37124},
  year = {2018},
  month = {sep},
  publisher = {{eLife} Sciences Publications Ltd},
  volume = {7},
  author = {Christopher M Kim and Carson C Chow},
  title = {{Learning recurrent dynamics in spiking networks}},
  journal = {{eLife}},
}


@article{Pascanu_2011,
  doi = {10.1016/j.neunet.2010.10.003},
  url = {https://doi.org/10.1016%2Fj.neunet.2010.10.003},
  year = {2011},
  month = {mar},
  publisher = {Elsevier {BV}},
  volume = {24},
  number = {2},
  pages = {199--207},
  author = {Razvan Pascanu and Herbert Jaeger},
  title = {{A neurodynamical model for working memory}},
  journal = {Neural Networks},
}


@inproceedings{Takimoto_2017,
  doi = {10.1109/devlrn.2017.8329796},
  url = {https://doi.org/10.1109%2Fdevlrn.2017.8329796},
  year = {2017},
  month = {sep},
  publisher = {{IEEE}},
  author = {Tomohiro Takimoto and Yuji Kawai and Jihoon Park and Minoru Asada},
  title = {{Self-organization based on auditory feedback promotes acquisition of babbling}},
  booktitle = {2017 Joint {IEEE} International Conference on Development and Learning and Epigenetic Robotics ({ICDL}-{EpiRob})},
}


@article{Savin_2014,
  doi = {10.3389/fncom.2014.00057},
  url = {https://doi.org/10.3389%2Ffncom.2014.00057},
  year = {2014},
  month = {may},
  publisher = {Frontiers Media {SA}},
  volume = {8},
  author = {Cristina Savin and Jochen Triesch},
  title = {{Emergence of task-dependent representations in working memory circuits}},
  journal = {Frontiers in Computational Neuroscience},
}


@article{Rajan_2016,
  doi = {10.1016/j.neuron.2016.02.009},
  url = {https://doi.org/10.1016%2Fj.neuron.2016.02.009},
  year = {2016},
  month = {apr},
  publisher = {Elsevier {BV}},
  volume = {90},
  number = {1},
  pages = {128--142},
  author = {Kanaka Rajan and Christopher~D. Harvey and David~W. Tank},
  title = {{Recurrent Network Models of Sequence Generation and Memory}},
  journal = {Neuron},
}


@inproceedings{Ceolini_2016,
  doi = {10.1109/ebccsp.2016.7605258},
  url = {https://doi.org/10.1109%2Febccsp.2016.7605258},
  year = {2016},
  month = {jun},
  publisher = {{IEEE}},
  author = {Enea Ceolini and Daniel Neil and Tobi Delbruck and Shih-Chii Liu},
  title = {{Temporal sequence recognition in a self-organizing recurrent network}},
  booktitle = {2016 Second International Conference on Event-based Control Communication, and Signal Processing ({EBCCSP})},
}


@article{herbert2017,
  howpublished = {http://www.jmlr.org/papers/v18/15-449.html},
  url = {http://www.jmlr.org/papers/v18/15-449.html},
  title = {{Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns}},
  note = {Accessed on Thu, November 01, 2018},
  author = {Jaeger, Herbert},
  journal = {Journsal of Machine Learning Research},
  year = {2017},
  volume = {18},
  issue = {13},
  pages = {1-43},
}


@article{Legenstein_2010,
  doi = {10.1523/jneurosci.4284-09.2010},
  url = {https://doi.org/10.1523%2Fjneurosci.4284-09.2010},
  year = {2010},
  month = {jun},
  publisher = {Society for Neuroscience},
  volume = {30},
  number = {25},
  pages = {8400--8410},
  author = {R. Legenstein and S. M. Chase and A. B. Schwartz and W. Maass},
  title = {{A Reward-Modulated Hebbian Learning Rule Can Explain Experimentally Observed Network Reorganization in a Brain Control Task}},
  journal = {Journal of Neuroscience},
}


@article{Asabuki_2018,
  doi = {10.1371/journal.pcbi.1006400},
  url = {https://doi.org/10.1371%2Fjournal.pcbi.1006400},
  year = {2018},
  month = {oct},
  publisher = {Public Library of Science ({PLoS})},
  volume = {14},
  number = {10},
  pages = {e1006400},
  author = {Toshitake Asabuki and Naoki Hiratani and Tomoki Fukai},
  editor = {Lyle J. Graham},
  title = {{Interactive reservoir computing for chunking information streams}},
  journal = {{PLOS} Computational Biology},
}


@article{Sussillo_2009,
  doi = {10.1016/j.neuron.2009.07.018},
  url = {https://doi.org/10.1016%2Fj.neuron.2009.07.018},
  year = {2009},
  month = {aug},
  publisher = {Elsevier {BV}},
  volume = {63},
  number = {4},
  pages = {544--557},
  author = {David Sussillo and L.F. Abbott},
  title = {{Generating Coherent Patterns of Activity from Chaotic Neural Networks}},
  journal = {Neuron},
}


@article{Mannella_2015,
  doi = {10.1007/s00422-015-0662-6},
  url = {https://doi.org/10.1007%2Fs00422-015-0662-6},
  year = {2015},
  month = {nov},
  publisher = {Springer Nature},
  volume = {109},
  number = {6},
  pages = {575--595},
  author = {Francesco Mannella and Gianluca Baldassarre},
  title = {{Selection of cortical dynamics for motor behaviour by the basal ganglia}},
  journal = {Biological Cybernetics},
}


@article{Dominey_1995,
  doi = {10.1007/bf00201428},
  url = {https://doi.org/10.1007%2Fbf00201428},
  year = {1995},
  month = {aug},
  publisher = {Springer Nature America Inc},
  volume = {73},
  number = {3},
  pages = {265--274},
  author = {Peter F. Dominey},
  title = {{Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning}},
  journal = {Biological Cybernetics},
}


@article{Pathak_2018,
  doi = {10.1103/physrevlett.120.024102},
  url = {https://doi.org/10.1103%2Fphysrevlett.120.024102},
  year = {2018},
  month = {jan},
  publisher = {American Physical Society ({APS})},
  volume = {120},
  number = {2},
  author = {Jaideep Pathak and Brian Hunt and Michelle Girvan and Zhixin Lu and Edward Ott},
  title = {{Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach}},
  journal = {Physical Review Letters},
}


@conference{Lowe2011,
  abstract = {This paper presents a biologically constrained reward prediction model capable of learning cue-outcome associations involving temporally distant stimuli without using the commonly used temporal difference model. The model incorporates a novel use of an adapted echo state network to substitute the biologically implausible delay chains usually used, in relation to dopamine phenomena, for tackling temporally structured stimuli. Moreover, the model is based on a novel algorithm which successfully coordinates two sub systems: one providing Pavlovian conditioning, one providing timely inhibition of dopamine responses to salient anticipated stimuli. The model is validated against the typical profile of phasic dopamine in first and second order Pavlovian conditioning. The model is relevant not only to explaining the mechanisms underlying the biological regulation of dopamine signals, but also for applications in autonomous robotics involving reinforcement-based learning.},
  author = {Lowe, Robert and Mannella, Francesco and Ziemke, Tom and Baldassarre, Gianluca},
  booktitle = {Advances in Artificial Life. Darwin Meets von Neumann. ECAL 2009. Lecture Notes in Computer Science, vol 5777},
  doi = {10.1007/978-3-642-21283-3_51},
  editor = {{Kampis G.} and {Karsai I.} and {Szathm{\'{a}}ry E.}},
  mendeley-groups = {RecurrentNetworks},
  pages = {410--417},
  publisher = {Springer, Berlin, Heidelberg},
  title = {{Modelling Coordination of Learning Systems: A Reservoir Systems Approach to Dopamine Modulated Pavlovian Conditioning}},
  url = {http://link.springer.com/10.1007/978-3-642-21283-3{\_}51},
  year = {2011},
}


@article{Laje2013,
  abstract = {The brain's ability to tell time and produce complex spatiotemporal motor patterns is critical for anticipating the next ring of a telephone or playing a musical instrument. One class of models proposes that these abilities emerge from dynamically changing patterns of neural activity generated in recurrent neural networks. However, the relevant dynamic regimes of recurrent networks are highly sensitive to noise; that is, chaotic. We developed a firing rate model that tells time on the order of seconds and generates complex spatiotemporal patterns in the presence of high levels of noise. This is achieved through the tuning of the recurrent connections. The network operates in a dynamic regime that exhibits coexisting chaotic and locally stable trajectories. These stable patterns function as 'dynamic attractors' and provide a feature that is characteristic of biological systems: the ability to 'return' to the pattern being generated in the face of perturbations.},
  author = {Laje, Rodrigo and Buonomano, Dean V},
  doi = {10.1038/nn.3405},
  issn = {1546-1726},
  journal = {Nature neuroscience},
  mendeley-groups = {RecurrentNetworks},
  month = {jul},
  number = {7},
  pages = {925--33},
  pmid = {23708144},
  title = {{Robust timing and motor patterns by taming chaos in recurrent neural networks.}},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/23708144},
  volume = {16},
  year = {2013},
}


@article{Sussilo2009,
  abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  author = {Sussillo, David and Abbott, L F},
  doi = {10.1016/j.neuron.2009.07.018},
  issn = {1097-4199},
  journal = {Neuron},
  mendeley-groups = {RecurrentNetworks},
  month = {aug},
  number = {4},
  pages = {544--57},
  pmid = {19709635},
  title = {{Generating coherent patterns of activity from chaotic neural networks.}},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/19709635},
  volume = {63},
  year = {2009},
}


@article{Brosch2015,
  abstract = {The processing of a visual stimulus can be subdivided into a number of stages. Upon stimulus presentation there is an early phase of feedforward processing where the visual information is propagated from lower to higher visual areas for the extraction of basic and complex stimulus features. This is followed by a later phase where horizontal connections within areas and feedback connections from higher areas back to lower areas come into play. In this later phase, image elements that are behaviorally relevant are grouped by Gestalt grouping rules and are labeled in the cortex with enhanced neuronal activity (object-based attention in psychology). Recent neurophysiological studies revealed that reward-based learning influences these recurrent grouping processes, but it is not well understood how rewards train recurrent circuits for perceptual organization. This paper examines the mechanisms for reward-based learning of new grouping rules. We derive a learning rule that can explain how rewards influence the information flow through feedforward, horizontal and feedback connections. We illustrate the efficiency with two tasks that have been used to study the neuronal correlates of perceptual organization in early visual cortex. The first task is called contour-integration and demands the integration of collinear contour elements into an elongated curve. We show how reward-based learning causes an enhancement of the representation of the to-be-grouped elements at early levels of a recurrent neural network, just as is observed in the visual cortex of monkeys. The second task is curve-tracing where the aim is to determine the endpoint of an elongated curve composed of connected image elements. If trained with the new learning rule, neural networks learn to propagate enhanced activity over the curve, in accordance with neurophysiological data. We close the paper with a number of model predictions that can be tested in future neurophysiological and computational studies.},
  author = {Brosch, Tobias and Neumann, Heiko and Roelfsema, Pieter R.},
  doi = {10.1371/journal.pcbi.1004489},
  editor = {Bethge, Matthias},
  issn = {1553-7358},
  journal = {PLOS Computational Biology},
  mendeley-groups = {RecurrentNetworks},
  month = {oct},
  number = {10},
  pages = {e1004489},
  publisher = {Public Library of Science},
  title = {{Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural Networks}},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1004489},
  volume = {11},
  year = {2015},
}


@article{Song2016,
  abstract = {The ability to simultaneously record from large numbers of neurons in behaving animals has ushered in a new era for the study of the neural circuit mechanisms underlying cognitive functions. One promising approach to uncovering the dynamical and computational principles governing population responses is to analyze model recurrent neural networks (RNNs) that have been optimized to perform the same tasks as behaving animals. Because the optimization of network parameters specifies the desired output but not the manner in which to achieve this output, “trained” networks serve as a source of mechanistic hypotheses and a testing ground for data analyses that link neural computation to behavior. Complete access to the activity and connectivity of the circuit, and the ability to manipulate them arbitrarily, make trained networks a convenient proxy for biological circuits and a valuable platform for theoretical investigation. However, existing RNNs lack basic biological features such as the distinction between excitatory and inhibitory units (Dale's principle), which are essential if RNNs are to provide insights into the operation of biological circuits. Moreover, trained networks can achieve the same behavioral performance but differ substantially in their structure and dynamics, highlighting the need for a simple and flexible framework for the exploratory training of RNNs. Here, we describe a framework for gradient descent-based training of excitatory-inhibitory RNNs that can incorporate a variety of biological knowledge. We provide an implementation based on the machine learning library Theano, whose automatic differentiation capabilities facilitate modifications and extensions. We validate this framework by applying it to well-known experimental paradigms such as perceptual decision-making, context-dependent integration, multisensory integration, parametric working memory, and motor sequence generation. Our results demonstrate the wide range of neural activity patterns and behavior that can be modeled, and suggest a unified setting in which diverse cognitive computations and mechanisms can be studied.},
  author = {Song, H. Francis and Yang, Guangyu R. and Wang, Xiao-Jing},
  doi = {10.1371/journal.pcbi.1004792},
  editor = {Sporns, Olaf},
  issn = {1553-7358},
  journal = {PLOS Computational Biology},
  mendeley-groups = {RecurrentNetworks},
  month = {feb},
  number = {2},
  pages = {e1004792},
  publisher = {Public Library of Science},
  title = {{Training Excitatory-Inhibitory Recurrent Neural Networks for Cognitive Tasks: A Simple and Flexible Framework}},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1004792},
  volume = {12},
  year = {2016},
}


@article{Sompolinsky1988,
  author = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  doi = {10.1103/PhysRevLett.61.259},
  issn = {0031-9007},
  journal = {Physical Review Letters},
  mendeley-groups = {RecurrentNetworks},
  month = {jul},
  number = {3},
  pages = {259--262},
  publisher = {American Physical Society},
  title = {{Chaos in Random Neural Networks}},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.61.259},
  volume = {61},
  year = {1988},
}


@article{Zhang2018,
  author = {Zhang, Zhewei and Cheng, Zhenbo and Lin, Zhongqiao and Nie, Chechang and Yang, Tianming},
  doi = {10.1371/journal.pcbi.1005925},
  editor = {Gershman, Samuel J.},
  issn = {1553-7358},
  journal = {PLOS Computational Biology},
  mendeley-groups = {RecurrentNetworks},
  month = {jan},
  number = {1},
  pages = {e1005925},
  publisher = {Public Library of Science},
  title = {{A neural network model for the orbitofrontal cortex and task space acquisition during reinforcement learning}},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1005925},
  volume = {14},
  year = {2018},
}


@inproceedings{Gallichio2016,
  abstract = {In this paper we propose an empirical analysis of deep recur-rent neural networks (RNNs) with stacked layers. The analysis aims at the study and proposal of approaches to develop and enhance multiple time-scale and hierarchical dynamics in deep recurrent architectures, within the efficient Reservoir Computing (RC) approach for RNN modeling. Results point out the actual relevance of layering and RC parameters aspects on the diversification of temporal representations in deep recurrent models.},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  booktitle = {European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  isbn = {9782875870278},
  mendeley-groups = {RecurrentNetworks},
  number = {April},
  pages = {27--29},
  title = {{Deep Reservoir Computing: A Critical Analysis}},
  year = {2016},
}


@article{Mastrogiuseppe2017,
  abstract = {Recurrent networks of non-linear units display a variety of dynamical regimes depending on the structure of their synaptic connectivity. A particularly remarkable phenomenon is the appearance of strongly fluctuating, chaotic activity in networks of deterministic, but randomly connected rate units. How this type of intrinsically generated fluctuations appears in more realistic networks of spiking neurons has been a long standing question. To ease the comparison between rate and spiking networks, recent works investigated the dynamical regimes of randomly-connected rate networks with segregated excitatory and inhibitory populations, and firing rates constrained to be positive. These works derived general dynamical mean field (DMF) equations describing the fluctuating dynamics, but solved these equations only in the case of purely inhibitory networks. Using a simplified excitatory-inhibitory architecture in which DMF equations are more easily tractable, here we show that the presence of excitation qualitatively modifies the fluctuating activity compared to purely inhibitory networks. In presence of excitation, intrinsically generated fluctuations induce a strong increase in mean firing rates, a phenomenon that is much weaker in purely inhibitory networks. Excitation moreover induces two different fluctuating regimes: for moderate overall coupling, recurrent inhibition is sufficient to stabilize fluctuations; for strong coupling, firing rates are stabilized solely by the upper bound imposed on activity, even if inhibition is stronger than excitation. These results extend to more general network architectures, and to rate networks receiving noisy inputs mimicking spiking activity. Finally, we show that signatures of the second dynamical regime appear in networks of integrate-and-fire neurons.},
  author = {Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  doi = {10.1371/journal.pcbi.1005498},
  editor = {Latham, Peter E.},
  issn = {1553-7358},
  journal = {PLOS Computational Biology},
  mendeley-groups = {RecurrentNetworks},
  month = {apr},
  number = {4},
  pages = {e1005498},
  publisher = {Public Library of Science},
  title = {{Intrinsically-generated fluctuating activity in excitatory-inhibitory networks}},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1005498},
  volume = {13},
  year = {2017},
}


@article{Enel2016,
  abstract = {Primates display a remarkable ability to adapt to novel situations. Determining what is most pertinent in these situations is not always possible based only on the current sensory inputs, and often also depends on recent inputs and behavioral outputs that contribute to internal states. Thus, one can ask how cortical dynamics generate representations of these complex situations. It has been observed that mixed selectivity in cortical neurons contributes to represent diverse situations defined by a combination of the current stimuli, and that mixed selectivity is readily obtained in randomly connected recurrent networks. In this context, these reservoir networks reproduce the highly recurrent nature of local cortical connectivity. Recombining present and past inputs, random recurrent networks from the reservoir computing framework generate mixed selectivity which provides pre-coded representations of an essentially universal set of contexts. These representations can then be selectively amplified through learning to solve the task at hand. We thus explored their representational power and dynamical properties after training a reservoir to perform a complex cognitive task initially developed for monkeys. The reservoir model inherently displayed a dynamic form of mixed selectivity, key to the representation of the behavioral context over time. The pre-coded representation of context was amplified by training a feedback neuron to explicitly represent this context, thereby reproducing the effect of learning and allowing the model to perform more robustly. This second version of the model demonstrates how a hybrid dynamical regime combining spatio-temporal processing of reservoirs, and input driven attracting dynamics generated by the feedback neuron, can be used to solve a complex cognitive task. We compared reservoir activity to neural activity of dorsal anterior cingulate cortex of monkeys which revealed similar network dynamics. We argue that reservoir computing is a pertinent framework to model local cortical dynamics and their contribution to higher cognitive function.},
  author = {Enel, Pierre and Procyk, Emmanuel and Quilodran, Ren{\'{e}} and Dominey, Peter Ford},
  doi = {10.1371/journal.pcbi.1004967},
  editor = {O'Reilly, Jill X},
  issn = {1553-7358},
  journal = {PLOS Computational Biology},
  mendeley-groups = {RecurrentNetworks},
  month = {jun},
  number = {6},
  pages = {e1004967},
  publisher = {Public Library of Science},
  title = {{Reservoir Computing Properties of Neural Dynamics in Prefrontal Cortex}},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1004967},
  volume = {12},
  year = {2016},
}


@article{Yamazaki2007,
  abstract = {We examined closely the cerebellar circuit model that we have proposed previously. The model granular layer generates a finite but very long sequence of active neuron populations without recurrence, which is able to represent the passage of time. For all the possible binary patterns fed into mossy fibres, the circuit generates the same number of different sequences of active neuron populations. Model Purkinje cells that receive parallel fiber inputs from neurons in the granular layer learn to stop eliciting spikes at the timing instructed by the arrival of signals from the inferior olive. These functional roles of the granular layer and Purkinje cells are regarded as a liquid state generator and readout neurons, respectively. Thus, the cerebellum that has been considered to date as a biological counterpart of a perceptron is reinterpreted to be a liquid state machine that possesses powerful information processing capability more than a perceptron.},
  author = {Yamazaki, Tadashi and Tanaka, Shigeru},
  doi = {10.1016/J.NEUNET.2007.04.004},
  issn = {0893-6080},
  journal = {Neural Networks},
  mendeley-groups = {RecurrentNetworks,Cerebellum},
  month = {apr},
  number = {3},
  pages = {290--297},
  publisher = {Pergamon},
  title = {{The cerebellum as a liquid state machine}},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608007000366?via{\%}3Dihub},
  volume = {20},
  year = {2007},
}


@article{Pitti2017,
  abstract = {The intra-parietal lobe coupled with the Basal Ganglia forms a working memory that demonstrates strong planning capabilities for generating robust yet flexible neuronal sequences. Neurocomputational models however, often fails to control long range neural synchrony in recurrent spiking networks due to spontaneous activity. As a novel framework based on the free-energy principle, we propose to see the problem of spikes' synchrony as an optimization problem of the neurons sub-threshold activity for the generation of long neuronal chains. Using a stochastic gradient descent, a reinforcement signal (presumably dopaminergic) evaluates the quality of one input vector to move the recurrent neural network to a desired activity; depending on the error made, this input vector is strengthened to hill-climb the gradient or elicited to search for another solution. This vector can be learned then by one associative memory as a model of the basal-ganglia to control the recurrent neural network. Experiments on habit learning and on sequence retrieving demonstrate the capabilities of the dual system to generate very long and precise spatio-temporal sequences, above two hundred iterations. Its features are applied then to the sequential planning of arm movements. In line with neurobiological theories, we discuss its relevance for modeling the cortico-basal working memory to initiate flexible goal-directed neuronal chains of causation and its relation to novel architectures such as Deep Networks, Neural Turing Machines and the Free-Energy Principle.},
  author = {Pitti, Alexandre and Gaussier, Philippe and Quoy, Mathias},
  doi = {10.1371/journal.pone.0173684},
  editor = {El-Deredy, Wael},
  issn = {1932-6203},
  journal = {PLOS ONE},
  mendeley-groups = {RecurrentNetworks},
  month = {mar},
  number = {3},
  pages = {e0173684},
  publisher = {Public Library of Science},
  title = {{Iterative free-energy optimization for recurrent neural networks (INFERNO)}},
  url = {http://dx.plos.org/10.1371/journal.pone.0173684},
  volume = {12},
  year = {2017},
}


@article{Murray2017,
  abstract = {Sparse, sequential patterns of neural activity have been observed in numerous brain areas during timekeeping and motor sequence tasks. Inspired by such observations, we construct a model of the striatum, an all-inhibitory circuit where sequential activity patterns are prominent, addressing the following key challenges: (i) obtaining control over temporal rescaling of the sequence speed, with the ability to generalize to new speeds; (ii) facilitating flexible expression of distinct sequences via selective activation, concatenation, and recycling of specific subsequences; and (iii) enabling the biologically plausible learning of sequences, consistent with the decoupling of learning and execution suggested by lesion studies showing that cortical circuits are necessary for learning, but that subcortical circuits are sufficient to drive learned behaviors. The same mechanisms that we describe can also be applied to circuits with both excitatory and inhibitory populations, and hence may underlie general features of sequential neural activity pattern generation in the brain.},
  author = {Murray, James M. and {Sean Escola}, G.},
  doi = {10.7554/eLife.26084.001},
  issn = {2050084X},
  journal = {eLife},
  mendeley-groups = {RecurrentNetworks},
  pmid = {28481200},
  title = {{Learning multiple variable-speed sequences in striatum via cortical tutoring}},
  volume = {6},
  year = {2017},
}


@article{Kuroki2017,
  abstract = {We have flexible control over our cognition depending on the context or surrounding environments. The prefrontal cortex (PFC) controls this cognitive flexibility; however, the detailed underlying mechanisms remain unclear. Recent developments in machine learning techniques have allowed simple recurrent neural network PFC models to perform human- or animal-like behavioral tasks. These systems allow us to acquire parameters, which we could not in biological experiments, for performing the tasks. We compared four models, in which a flexible cognition task, called context-dependent integration task, was performed; subsequently, we searched for common features. In all the models, we observed that high plastic synapses were concentrated in the small neuronal population and the more concentrated neuronal units contributed further to the performance. However, there were no common properties in the constructed structures. These results suggest that plastic changes can be more general and important to accomplish cognitive tasks than features of the constructed structures.},
  author = {Kuroki, Satoshi and Isomura, Takuya},
  doi = {10.1101/181297},
  journal = {bioRxiv},
  mendeley-groups = {RecurrentNetworks},
  month = {aug},
  pages = {181297},
  publisher = {Cold Spring Harbor Laboratory},
  title = {{Common features in plastic changes rather than constructed structures in recurrent neural network prefrontal cortex models}},
  url = {https://www.biorxiv.org/content/early/2017/08/28/181297},
  year = {2017},
}


@article{Hinaut2014,
  abstract = {One of the principal functions of human language is to allow people to coordinate joint action. This includes the description of events, requests for action, and their organization in time. A crucial component of language acquisition is learning the grammatical structures that allow the expression of such complex meaning related to physical events. The current research investigates the learning of grammatical constructions and their temporal organization in the context of human-robot physical interaction with the embodied sensorimotor humanoid platform, the iCub. We demonstrate three noteworthy phenomena. First, a recurrent network model is used in conjunction with this robotic platform to learn the mappings between grammatical forms and predicate-argument representations of meanings related to events, and the robot's execution of these events in time. Second, this learning mechanism functions in the inverse sense, i.e. in a language production mode, where rather than executing commanded actions, the robot will describe the results of human generated actions. Finally, we collect data from na{\"{i}}ve subjects who interact with the robot via spoken language, and demonstrate significant learning and generalization results. This allows us to conclude that such a neural language learning system not only helps to characterize and understand some aspects of human language acquisition, but also that it can be useful in adaptive human-robot interaction.},
  author = {Hinaut, Xavier and Petit, Maxime and Pointeau, Gregoire and Dominey, Peter Ford},
  doi = {10.3389/fnbot.2014.00016},
  file = {::},
  issn = {1662-5218},
  journal = {Frontiers in Neurorobotics},
  keywords = {Echo state networks,Language production,anytime,grammatical constructions,human-robot interaction,iCub Humanoid,language acquisition,language model,recurrent neural networks,reservoir computing},
  mendeley-groups = {RecurrentNetworks},
  month = {may},
  pages = {16},
  publisher = {Frontiers},
  title = {{Exploring the acquisition and production of grammatical constructions through human-robot interaction with echo state networks}},
  url = {http://journal.frontiersin.org/article/10.3389/fnbot.2014.00016/abstract},
  volume = {8},
  year = {2014},
}


@article{Roessert2015,
  abstract = {Models of the cerebellar microcircuit often assume that input signals from the mossy-fibers are expanded and recoded to provide a foundation from which the Purkinje cells can synthesize output filters to implement specific input-signal transformations. Details of this process are however unclear. While previous work has shown that recurrent granule cell inhibition could in principle generate a wide variety of random outputs suitable for coding signal onsets, the more general application for temporally varying signals has yet to be demonstrated. Here we show for the first time that using a mechanism very similar to reservoir computing enables random neuronal networks in the granule cell layer to provide the necessary signal separation and extension from which Purkinje cells could construct basis filters of various time-constants. The main requirement for this is that the network operates in a state of criticality close to the edge of random chaotic behavior. We further show that the lack of recurrent excitation in the granular layer as commonly required in traditional reservoir networks can be circumvented by considering other inherent granular layer features such as inverted input signals or mGluR2 inhibition of Golgi cells. Other properties that facilitate filter construction are direct mossy fiber excitation of Golgi cells, variability of synaptic weights or input signals and output-feedback via the nucleocortical pathway. Our findings are well supported by previous experimental and theoretical work and will help to bridge the gap between system-level models and detailed models of the granular layer network.},
  author = {R{\"{o}}ssert, Christian and Dean, Paul and Porrill, John},
  doi = {10.1371/journal.pcbi.1004515},
  editor = {Graham, Lyle J.},
  issn = {1553-7358},
  journal = {PLOS Computational Biology},
  mendeley-groups = {RecurrentNetworks},
  month = {oct},
  number = {10},
  pages = {e1004515},
  publisher = {Public Library of Science},
  title = {{At the Edge of Chaos: How Cerebellar Granular Layer Network Dynamics Can Provide the Basis for Temporal Filters}},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1004515},
  volume = {11},
  year = {2015},
}


@article{Venkatesh2018,
  title = {{Brain dynamics and temporal trajectories during task and naturalistic processing.}},
  date = {2018 Nov 16},
  source = {Neuroimage},
  authors = {Venkatesh, M and Jaja, J and Pessoa, L},
  author = {Venkatesh, M and Jaja, J and Pessoa, L},
  year = {2018},
  month = {Nov},
  journal = {Neuroimage},
  volume = {186},
  pages = {410-423},
  pubmed_id = {30453032},
  doi = {10.1016/j.neuroimage.2018.11.016},
}
