<h1 data-label="761552" class="ltx_title_section"> Introduction</h1><div></div><h2 data-label="422720" class="ltx_title_subsection">What is reservoir computing?</h2><div></div><ul><li>Echo-state networks&nbsp;<cite class="ltx_cite raw v1">\cite{Jaeger_2007}</cite>&nbsp; / The “echo state” approach to analysing and training recurrent neural networks – with an Erratum note, H. Jaeger 2010 (<a href="https://www.researchgate.net/profile/Herbert_Jaeger3/publication/215385037_The_echo_state_approach_to_analysing_and_training_recurrent_neural_networks-with_an_erratum_note'/links/566a003508ae62b05f027be3/The-echo-state-approach-to-analysing-and-training-recurrent-neural-networks-with-an-erratum-note.pdf">https://www.researchgate.net/profile/Herbert_Jaeger3/publication/215385037_The_echo_state_approach_to_analysing_and_training_recurrent_neural_networks-with_an_erratum_note'/links/566a003508ae62b05f027be3/The-echo-state-approach-to-analysing-and-training-recurrent-neural-networks-with-an-erratum-note.pdf</a>)</li><li>Liquid-state machines <cite class="ltx_cite raw v1">\cite{Maass_2002}</cite></li><li>Lay out difference to deep learning, where deep learning can be a proper tool to model brain networks and where current deep learning architectures have a deficiency in modeling brain structures/phenomena&nbsp;</li></ul><h2 data-label="331158" class="ltx_title_subsection">Definitions:</h2><div></div><ul><li><cite class="ltx_cite raw v1">\cite{Jaeger}</cite>:&nbsp; RNN learning architecture defined by 1)&nbsp;a large, randomly connected, recurrent “reservoir”&nbsp;network&nbsp;that is passively excited by the task's input signal, and</li></ul><div></div>