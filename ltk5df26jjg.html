<h1 data-label="761552" class="ltx_title_section"> Introduction</h1><div></div><h2 data-label="422720" class="ltx_title_subsection">What is reservoir computing?</h2><div></div><ul><li>Echo-state networks&nbsp;<cite class="ltx_cite raw v1">\cite{Jaeger_2007}</cite>&nbsp; / The “echo state” approach to analysing and training recurrent neural networks – with an Erratum note, H. Jaeger 2010 (<a href="https://www.researchgate.net/profile/Herbert_Jaeger3/publication/215385037_The_echo_state_approach_to_analysing_and_training_recurrent_neural_networks-with_an_erratum_note'/links/566a003508ae62b05f027be3/The-echo-state-approach-to-analysing-and-training-recurrent-neural-networks-with-an-erratum-note.pdf">https://www.researchgate.net/profile/Herbert_Jaeger3/publication/215385037_The_echo_state_approach_to_analysing_and_training_recurrent_neural_networks-with_an_erratum_note'/links/566a003508ae62b05f027be3/The-echo-state-approach-to-analysing-and-training-recurrent-neural-networks-with-an-erratum-note.pdf</a>)</li><li>Liquid-state machines <cite class="ltx_cite raw v1">\cite{Maass_2002}</cite></li></ul><div></div><h2 data-label="923846" class="ltx_title_subsection">Existing reviews on reservoir computing</h2><div></div><ul><li>Future directions for ESNs <cite class="ltx_cite raw v1">\cite{Jaeger}</cite></li><li>Special issue on echo state networks and liquid state machines <cite class="ltx_cite raw v1">\cite{Jaeger_2007a}</cite></li><li>Reservoir computing approaches to recurrent neural network training <cite class="ltx_cite raw v1">\cite{Luko_evi_ius_2009}</cite></li><li>Reservoir computing trends <cite class="ltx_cite raw v1">\cite{Luko_evi_ius_2012}</cite></li><li>Liquid state machines: Motivation, theory and applications <cite class="ltx_cite raw v1">\cite{Maass_2011}</cite></li></ul><div></div><h2 data-label="389123" class="ltx_title_subsection">Motivation for and purpose of this review</h2><div></div><h3 data-label="211185" class="ltx_title_subsubsection">Motivation</h3><div>Wide range of studies that bring together RC principles and neuroscientific questions and vice versa.</div><div>What is the collective picture so far that those studies provide of brain function from the perspective of reservoir computing?</div><ul><li>which aspects of neural signalling dynamics can be addressed by RC?</li><li>which aspects of brain function can be modeled via RC?</li><li>what might be the neural substrates of RC principles/mechanisms?</li></ul><div></div><h3 data-label="907478" class="ltx_title_subsubsection">Goal</h3><div>Developing an explicit account of how the brain implements and employs reservoir computing principles</div><div></div><h1 data-label="748924" class="ltx_title_section">Reservoir Computing as a Model for Sequence Learning in the Brain</h1><div></div><h2 data-label="877560" class="ltx_title_subsection">Supervised learning</h2><ul><li>Linear read-out of ESNs and LSMs <cite class="ltx_cite raw v1">\cite{Jaeger_2007,Maass_2002,Luko_evi_ius_2009}</cite></li><li>Gradient-descent methods (Backprop through time&nbsp;<cite class="ltx_cite raw v1">\cite{Werbos_1990}</cite>, APRL&nbsp;<cite class="ltx_cite raw v1">\cite{Atiya_2000}</cite>, backpropagation-decorrelation <cite class="ltx_cite raw v1">\cite{Steil,Steil_2006}</cite>)</li><li>Entropy-minimization&nbsp;<cite class="ltx_cite raw v1">\cite{Galtier_2014}</cite></li></ul><h2 data-label="600188" class="ltx_title_subsection">Unsupervised learning</h2><ul><li>Structural plasticity (STDP <cite class="ltx_cite raw v1">\cite{Klampfl_2013}</cite>,&nbsp; hebbian learning <cite class="ltx_cite raw v1">\cite{Bourjaily_2011}</cite>)</li><li>Intrinsic plasticity mechanisms (SORN&nbsp;<cite class="ltx_cite raw v1">\cite{Lazar_2011,Lazar_2009}</cite>, bayesian inference <cite class="ltx_cite raw v1">\cite{Kappel_2015,Kappel_2018}</cite>)</li></ul><h2 data-label="893657" class="ltx_title_subsection">Weakly-supervised learning</h2><ul><li>Reinforcement learning (RM-SORN&nbsp;<cite class="ltx_cite raw v1">\cite{Aswolinskiy_2015}</cite>, delayed phasic rewards&nbsp;<cite class="ltx_cite raw v1">\cite{Miconi2017}</cite>, reward-based learning tasks <cite class="ltx_cite raw v1">\cite{Kappel_2018}</cite>)</li><li>Evolutionary algorithms ()</li></ul><div>Each section should address the questions: </div><ul><li>What can we and what can we not infer about sequence learning in the brain by using this kind of learning rule in a model?</li><li>Which neural computations need to be performed and how could they be implemented in the brain?</li></ul><div></div><div><cite class="ltx_cite raw v1">\citep{Miconi_2017}</cite></div>